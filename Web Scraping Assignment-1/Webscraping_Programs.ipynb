{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3454e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c41afa7",
   "metadata": {},
   "source": [
    "# 1) Write a python program to display all the header tags from wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beff9bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1 class=\"firstHeading\" id=\"firstHeading\">Main Page</h1>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>,\n",
       " <h2>Navigation menu</h2>,\n",
       " <h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-personal-label\">\n",
       " <span>Personal tools</span>\n",
       " </h3>,\n",
       " <h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-namespaces-label\">\n",
       " <span>Namespaces</span>\n",
       " </h3>,\n",
       " <h3 aria-label=\"Change language variant\" class=\"vector-menu-heading\" id=\"p-variants-label\">\n",
       " <span>Variants</span>\n",
       " <span class=\"vector-menu-checkbox-expanded\">expanded</span>\n",
       " <span class=\"vector-menu-checkbox-collapsed\">collapsed</span>\n",
       " </h3>,\n",
       " <h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-views-label\">\n",
       " <span>Views</span>\n",
       " </h3>,\n",
       " <h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-cactions-label\">\n",
       " <span>More</span>\n",
       " <span class=\"vector-menu-checkbox-expanded\">expanded</span>\n",
       " <span class=\"vector-menu-checkbox-collapsed\">collapsed</span>\n",
       " </h3>,\n",
       " <h3>\n",
       " <label for=\"searchInput\">Search</label>\n",
       " </h3>,\n",
       " <h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-navigation-label\">\n",
       " <span>Navigation</span>\n",
       " </h3>,\n",
       " <h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-interaction-label\">\n",
       " <span>Contribute</span>\n",
       " </h3>,\n",
       " <h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-tb-label\">\n",
       " <span>Tools</span>\n",
       " </h3>,\n",
       " <h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-coll-print_export-label\">\n",
       " <span>Print/export</span>\n",
       " </h3>,\n",
       " <h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-wikibase-otherprojects-label\">\n",
       " <span>In other projects</span>\n",
       " </h3>,\n",
       " <h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-lang-label\">\n",
       " <span>Languages</span>\n",
       " </h3>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://en.wikipedia.org/wiki/Main_Page\n",
    "def header_tags(url):\n",
    "    m_url = requests.get(url)\n",
    "    soup = BeautifulSoup(m_url.content,\"html.parser\")\n",
    "    header_tags = soup.find_all(['h1','h2','h3','h4','h4','h5'])\n",
    "    return header_tags\n",
    "\n",
    "\n",
    "header_tags(url=\"https://en.wikipedia.org/wiki/Main_Page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77d36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b346e595",
   "metadata": {},
   "source": [
    "# 2) Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6aa9ce99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ===> https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&start=0&ref_=adv_nxt\n",
      "51 ===> https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&start=51&ref_=adv_nxt\n",
      "                           Movie_Name Release_Year Rating\n",
      "0            The Shawshank Redemption         1994    9.3\n",
      "1                       The Godfather         1972    9.2\n",
      "2                     The Dark Knight         2008    9.0\n",
      "3              The Godfather: Part II         1974    9.0\n",
      "4                        12 Angry Men         1957    9.0\n",
      "..                                ...          ...    ...\n",
      "95                            Vertigo         1958    8.3\n",
      "96                Singin' in the Rain         1952    8.3\n",
      "97                Ladri di biciclette         1948    8.3\n",
      "98                       Citizen Kane         1941    8.3\n",
      "99  M - Eine Stadt sucht einen Mörder         1931    8.3\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def imbd_movie_list():\n",
    "    movies_list = []\n",
    "    for j in range(0,102,51):\n",
    "        url = f\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&start={j}&ref_=adv_nxt\"\n",
    "        print(j,\"===>\",url)\n",
    "        data = requests.get(url)\n",
    "        soup = BeautifulSoup(data.content,\"html.parser\")\n",
    "        li = soup.find_all('div',class_=\"lister-item mode-advanced\")\n",
    "        \n",
    "        for m,i in enumerate(li):\n",
    "            movie_name = i.find('img', alt=True)\n",
    "            year = i.find('span',class_ = \"lister-item-year text-muted unbold\").get_text()\n",
    "            rating = i.find('div',class_ = \"inline-block ratings-imdb-rating\").get_text()\n",
    "            #print(movie_translation)\n",
    "            val  = {\"Movie_Name\":movie_name['alt'],\"Release_Year\":year.strip(\"()\"),\"Rating\":rating.strip()}\n",
    "            movies_list.append(val)\n",
    "    df = pd.DataFrame(movies_list)\n",
    "    print(df)\n",
    "\n",
    "imbd_movie_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdca264",
   "metadata": {},
   "source": [
    "# 3) Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of\n",
    "release) and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abea06ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Movie_Rank        Movie_Names Release_Year  Ratings\n",
      "0           1           Jai Bhim         2021      8.6\n",
      "1           2            Nayakan         1987      8.5\n",
      "2           3  Pariyerum Perumal         2018      8.5\n",
      "3           4         Anbe Sivam         2003      8.5\n",
      "4           5  C/o Kancharapalem         2018      8.5\n",
      "..        ...                ...          ...      ...\n",
      "95         96          Sarfarosh         1999      8.1\n",
      "96         97              Queen         2013      8.1\n",
      "97         98               Roja         1992      8.1\n",
      "98         99    OMG: Oh My God!         2012      8.1\n",
      "99        100     Dil Chahta Hai         2001      8.1\n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.imdb.com/india/top-rated-indian-movies/\n",
    "def IMDB_100_Indian_movies(url):\n",
    "    tables = pd.read_html(url)\n",
    "    titles = tables[0]['Rank & Title']\n",
    "    ratings = tables[0]['IMDb Rating']\n",
    "    movie_rank = []\n",
    "    movie_names = []\n",
    "    release_year = []\n",
    "    for i in titles:\n",
    "        rank = i.split()[0].strip('.')\n",
    "        year = i.split()[-1].strip('()')\n",
    "        name = \" \".join(i.split()[1:-1])\n",
    "        movie_rank.append(rank)\n",
    "        movie_names.append(name)\n",
    "        release_year.append(year)\n",
    "    df = pd.DataFrame(list(zip(movie_rank ,movie_names ,release_year,ratings)),columns = ['Movie_Rank' ,'Movie_Names' ,'Release_Year','Ratings']) \n",
    "    print(df[:100])\n",
    "    return \"success\"\n",
    "\n",
    "IMDB_100_Indian_movies(url=\"https://www.imdb.com/india/top-rated-indian-movies/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e9ab91",
   "metadata": {},
   "source": [
    "# 4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "    a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "    b) Top 10 ODI Batsmen in men along with the records of their team and rating.\n",
    "    c) Top 10 ODI bowlers along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11568b30",
   "metadata": {},
   "source": [
    "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5a6909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pos           Team  T  Matches  M  Points  P  Rating  R\n",
      "0    1   New Zealand  NZ          17       2054        121\n",
      "1    2      England  ENG          32       3793        119\n",
      "2    3    Australia  AUS          28       3244        116\n",
      "3    4        India  IND          32       3624        113\n",
      "4    5  South Africa  SA          25       2459         98\n",
      "5    6     Pakistan  PAK          27       2524         93\n",
      "6    7   Bangladesh  BAN          30       2740         91\n",
      "7    8   West Indies  WI          30       2523         84\n",
      "8    9     Sri Lanka  SL          32       2657         83\n",
      "9   10  Afghanistan  AFG          17       1054         62\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.icc-cricket.com/rankings/mens/team-rankings/odi\n",
    "def top_10_ODI_teams_men(url):\n",
    "    table = pd.read_html(url)\n",
    "    df = pd.DataFrame(table[0])\n",
    "    print(df[:10])\n",
    "    return \"success\"\n",
    "\n",
    "top_10_ODI_teams_men(url=\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88bf7a9",
   "metadata": {},
   "source": [
    "# b) Top 10 ODI Batsmen in men along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d174873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Player Team Rating\n",
      "0       Babar Azam  PAK    873\n",
      "1      Virat Kohli  IND    844\n",
      "2     Rohit Sharma  IND    813\n",
      "3      Ross Taylor   NZ    801\n",
      "4      Aaron Finch  AUS    779\n",
      "5   Jonny Bairstow  ENG    775\n",
      "6     David Warner  AUS    762\n",
      "7        Shai Hope   WI    758\n",
      "8  Kane Williamson   NZ    754\n",
      "9  Quinton de Kock   SA    743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.icc-cricket.com/rankings/mens/player-rankings/odi\n",
    "def top_10_ODI_Batsmen_men(url):\n",
    "    data = requests.get(url)\n",
    "    soup = BeautifulSoup(data.content,\"html.parser\")\n",
    "    name = soup.find(\"div\",attrs={'class':\"rankings-block__banner--name\"}).get_text().strip()\n",
    "    team = soup.find(\"div\",attrs={'class':\"rankings-block__banner--nationality\"}).get_text().strip()\n",
    "    rating = soup.find(\"div\",attrs={'class':\"rankings-block__banner--rating\"}).get_text()\n",
    "    \n",
    "    df = pd.DataFrame({\"Player\": name, \"Team\": team,\"Rating\":rating}, index=[0])\n",
    "    \n",
    "    tables = pd.read_html(url,attrs = {'class' : 'table rankings-card-table'})\n",
    "    df1 = pd.DataFrame(tables[0])\n",
    "    df1.drop(columns = 'Pos',inplace=True)\n",
    "    df = df.append(df1,ignore_index=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(df)\n",
    "    return 'success'\n",
    "\n",
    "top_10_ODI_Batsmen_men(url=\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f9ef63",
   "metadata": {},
   "source": [
    "# c) Top 10 ODI bowlers along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "901f730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Player Team Rating\n",
      "0       Trent Boult   NZ    737\n",
      "1    Josh Hazlewood  AUS    709\n",
      "2  Mujeeb Ur Rahman  AFG    708\n",
      "3      Chris Woakes  ENG    700\n",
      "4      Mehedi Hasan  BAN    692\n",
      "5        Matt Henry   NZ    691\n",
      "6    Jasprit Bumrah  IND    679\n",
      "7    Mitchell Starc  AUS    652\n",
      "8   Shakib Al Hasan  BAN    650\n",
      "9     Kagiso Rabada   SA    643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.icc-cricket.com/rankings/mens/player-rankings/odi\n",
    "def top_10_ODI_Bowlers_men(url):\n",
    "    data = requests.get(url)\n",
    "    soup = BeautifulSoup(data.content,\"html.parser\")\n",
    "    Team = soup.find_all(\"div\",attrs={'class':\"rankings-block__banner--nationality\"})\n",
    "    rating = soup.find_all(\"div\",attrs={'class':\"rankings-block__banner--rating\"})\n",
    "    name = soup.find_all(\"div\",attrs={'class':\"rankings-block__banner--name\"})\n",
    "    \n",
    "    p_name = [i.get_text().strip() for n,i in enumerate(name) if n==1]\n",
    "    p_rating = [i.get_text().strip() for n,i in enumerate(rating) if n==1]\n",
    "    p_team = [i.get_text().strip() for n,i in enumerate(Team) if n==1]\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(p_name,p_team,p_rating)),columns=['Player','Team','Rating'])\n",
    "    \n",
    "    tables = pd.read_html(url,attrs = {'class' : 'table rankings-card-table'})\n",
    "    df1 = pd.DataFrame(tables[1])\n",
    "    df1.drop(columns = 'Pos',inplace=True)\n",
    "    df = df.append(df1,ignore_index=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(df)\n",
    "    return 'success'\n",
    "\n",
    "top_10_ODI_Bowlers_men(url=\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d07be8b",
   "metadata": {},
   "source": [
    "# 5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "    \n",
    "    a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "    b) Top 10 women’s ODI players along with the records of their team and rating.\n",
    "    c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116cfa99",
   "metadata": {},
   "source": [
    "# a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "569e193c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pos           Team  T  Matches  M  Points  P  Rating  R\n",
      "0    1    Australia  AUS          17       2746        162\n",
      "1    2  South Africa  SA          19       2307        121\n",
      "2    3      England  ENG          18       2148        119\n",
      "3    4        India  IND          17       1899        112\n",
      "4    5   Bangladesh  BAN           5        475         95\n",
      "5    6   New Zealand  NZ          19       1668         88\n",
      "6    7   West Indies  WI          19       1658         87\n",
      "7    8     Pakistan  PAK          18       1226         68\n",
      "8    9      Ireland  IRE           5        240         48\n",
      "9   10     Sri Lanka  SL           5        233         47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.icc-cricket.com/rankings/womens/team-rankings/odi\n",
    "def top_10_ODI_teams_women(url):\n",
    "    table = pd.read_html(url)\n",
    "    df = pd.DataFrame(table[0])\n",
    "    print(df[:10])\n",
    "    return \"success\"\n",
    "\n",
    "top_10_ODI_teams_women(url=\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cfcba7",
   "metadata": {},
   "source": [
    "# b) Top 10 women’s ODI players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "750d25c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Player Team Rating\n",
      "0        Lizelle Lee   SA    761\n",
      "1       Alyssa Healy  AUS    750\n",
      "2        Mithali Raj  IND    738\n",
      "3     Tammy Beaumont  ENG    728\n",
      "4  Amy Satterthwaite   NZ    717\n",
      "5    Smriti Mandhana  IND    710\n",
      "6        Meg Lanning  AUS    699\n",
      "7        Beth Mooney  AUS    690\n",
      "8    Stafanie Taylor   WI    676\n",
      "9     Heather Knight  ENG    674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.icc-cricket.com/rankings/womens/player-rankings/odi\n",
    "def top_10_ODI_Batsmen_women(url):\n",
    "    data = requests.get(url)\n",
    "    soup = BeautifulSoup(data.content,\"html.parser\")\n",
    "    name = soup.find(\"div\",attrs={'class':\"rankings-block__banner--name\"}).get_text().strip()\n",
    "    team = soup.find(\"div\",attrs={'class':\"rankings-block__banner--nationality\"}).get_text().strip()\n",
    "    rating = soup.find(\"div\",attrs={'class':\"rankings-block__banner--rating\"}).get_text()\n",
    "    \n",
    "    df = pd.DataFrame({\"Player\": name, \"Team\": team,\"Rating\":rating}, index=[0])\n",
    "    \n",
    "    tables = pd.read_html(url,attrs = {'class' : 'table rankings-card-table'})\n",
    "    df1 = pd.DataFrame(tables[0])\n",
    "    df1.drop(columns = 'Pos',inplace=True)\n",
    "    df = df.append(df1,ignore_index=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(df)\n",
    "    return 'success'\n",
    "\n",
    "top_10_ODI_Batsmen_women(url=\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71010722",
   "metadata": {},
   "source": [
    "# c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7147436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Player Team Rating\n",
      "0      Jess Jonassen  AUS    760\n",
      "1     Jhulan Goswami  IND    727\n",
      "2       Megan Schutt  AUS    717\n",
      "3     Marizanne Kapp   SA    715\n",
      "4  Sophie Ecclestone  ENG    701\n",
      "5     Shabnim Ismail   SA    688\n",
      "6    Katherine Brunt  ENG    666\n",
      "7     Ayabonga Khaka   SA    643\n",
      "8     Anya Shrubsole  ENG    598\n",
      "9         Kate Cross  ENG    589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.icc-cricket.com/rankings/womens/player-rankings/odi\n",
    "def top_10_ODI_Bowlers_women(url):\n",
    "    data = requests.get(url)\n",
    "    soup = BeautifulSoup(data.content,\"html.parser\")\n",
    "    Team = soup.find_all(\"div\",attrs={'class':\"rankings-block__banner--nationality\"})\n",
    "    rating = soup.find_all(\"div\",attrs={'class':\"rankings-block__banner--rating\"})\n",
    "    name = soup.find_all(\"div\",attrs={'class':\"rankings-block__banner--name\"})\n",
    "    \n",
    "    p_name = [i.get_text().strip() for n,i in enumerate(name) if n==1]\n",
    "    p_rating = [i.get_text().strip() for n,i in enumerate(rating) if n==1]\n",
    "    p_team = [i.get_text().strip() for n,i in enumerate(Team) if n==1]\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(p_name,p_team,p_rating)),columns=['Player','Team','Rating'])\n",
    "    \n",
    "    tables = pd.read_html(url,attrs = {'class' : 'table rankings-card-table'})\n",
    "    df1 = pd.DataFrame(tables[1])\n",
    "    df1.drop(columns = 'Pos',inplace=True)\n",
    "    df = df.append(df1,ignore_index=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(df)\n",
    "    return 'success'\n",
    "\n",
    "top_10_ODI_Bowlers_women(url=\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1481ff6d",
   "metadata": {},
   "source": [
    "# 6) Write a python program to scrape details of all the posts from coreyms.com. Scrape the heading, date, content and the code for the video from the link for the youtube video from the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ff16d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "https://coreyms.com/page/1\n",
      "https://coreyms.com/page/2\n",
      "https://coreyms.com/page/3\n",
      "https://coreyms.com/page/4\n",
      "https://coreyms.com/page/5\n",
      "https://coreyms.com/page/6\n",
      "https://coreyms.com/page/7\n",
      "https://coreyms.com/page/8\n",
      "https://coreyms.com/page/9\n",
      "https://coreyms.com/page/10\n",
      "https://coreyms.com/page/11\n",
      "https://coreyms.com/page/12\n",
      "https://coreyms.com/page/13\n",
      "https://coreyms.com/page/14\n",
      "https://coreyms.com/page/15\n",
      "https://coreyms.com/page/16\n",
      "https://coreyms.com/page/17\n",
      "Before deleting empty rows :  190\n",
      "After deleting empty rows :  160\n",
      "                                              Headings                Date  \\\n",
      "0    Python Tutorial: Zip Files – Creating and Extr...   November 19, 2019   \n",
      "1    Python Data Science Tutorial: Analyzing the 20...    October 17, 2019   \n",
      "2    Python Multiprocessing Tutorial: Run Code in P...  September 21, 2019   \n",
      "3    Python Threading Tutorial: Run Code Concurrent...  September 12, 2019   \n",
      "4                                  Update (2019-09-03)   September 3, 2019   \n",
      "..                                                 ...                 ...   \n",
      "155    Quick Tip: Use a Wooden Pallet as a Lumber Rack      April 21, 2014   \n",
      "156  How to Record Sound From Your Computer’s Speak...       March 2, 2014   \n",
      "157                            Ems and Rems for Sizing  September 21, 2013   \n",
      "158                             How to Build a Pergola  September 12, 2013   \n",
      "159  Adding Custom Web Fonts to Your Web Site Using...        July 1, 2013   \n",
      "\n",
      "                                               Content  \\\n",
      "0    In this video, we will be learning how to crea...   \n",
      "1    In this Python Programming video, we will be l...   \n",
      "2    In this Python Programming video, we will be l...   \n",
      "3    In this Python Programming video, we will be l...   \n",
      "4    Hey everyone. I wanted to give you an update o...   \n",
      "..                                                 ...   \n",
      "155  Making a lumber rack for the workshop is a tas...   \n",
      "156  At times, you may want to record sound from yo...   \n",
      "157  What are Ems and Rems?\\nEms and Rems both are ...   \n",
      "158  Building a pergola is a fun and easy project. ...   \n",
      "159  Adding custom web fonts to your website is one...   \n",
      "\n",
      "                                          Youtube_Link  \n",
      "0    https://www.youtube.com/embed/z0gguhEmWiY?vers...  \n",
      "1    https://www.youtube.com/embed/_P7X8tMplsw?vers...  \n",
      "2    https://www.youtube.com/embed/fKl2JW_qrso?vers...  \n",
      "3    https://www.youtube.com/embed/IEEhzQoKtQU?vers...  \n",
      "4                                                       \n",
      "..                                                 ...  \n",
      "155                                                     \n",
      "156                                                     \n",
      "157                                                     \n",
      "158                                                     \n",
      "159  https://www.youtube.com/embed/y2AlgMII1OU?vers...  \n",
      "\n",
      "[160 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def coreyms_website(url):\n",
    "    \n",
    "    data = requests.get(url)\n",
    "    soup = BeautifulSoup(data.content,\"html.parser\")\n",
    "    pagination = soup.find('div',class_=\"archive-pagination pagination\")\n",
    "    page_number = []\n",
    "    for i in soup.find('div',class_=\"archive-pagination pagination\"):\n",
    "        u = i.find_all('a',href=True)\n",
    "        print(u[-2].get_text())\n",
    "        page_number.append(u[-2].get_text())\n",
    "    data = []\n",
    "    for page_ in range(1,int(page_number[0])+1):\n",
    "        #print(url+f\"page/{page_}\")\n",
    "        in_url = url+f\"page/{page_}\"\n",
    "        print(in_url)\n",
    "        in_data = requests.get(in_url)\n",
    "        in_soup = BeautifulSoup(in_data.content,\"html.parser\")\n",
    "        in_class = in_soup.find('main',class_='content')\n",
    "        for i in in_class:\n",
    "            try:\n",
    "                heading =i.find('a',href=True).get_text()\n",
    "            except:\n",
    "                heading = \"\"\n",
    "            try:\n",
    "                time = i.find('time',class_='entry-time').get_text()\n",
    "            except:\n",
    "                time = \"\"\n",
    "            try:\n",
    "                content = i.find('div',class_='entry-content').get_text().strip()\n",
    "            except:\n",
    "                content = \"\"\n",
    "            try:\n",
    "                youtube_link = i.find('iframe',class_='youtube-player')['src']\n",
    "            except:\n",
    "                youtube_link = \"\"\n",
    "            val = {\"Headings\":heading,\"Date\":time,\"Content\":content,\"Youtube_Link\":youtube_link}\n",
    "            data.append(val)\n",
    "    df = pd.DataFrame(data)   \n",
    "    print(\"Before deleting empty rows : \",len(df))\n",
    "    df['Content'].replace(\"\",np.nan,inplace=True)\n",
    "    df.dropna(subset=['Content'], inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(\"After deleting empty rows : \",len(df))\n",
    "    print(df)\n",
    "    \n",
    "    return \"success\"\n",
    "\n",
    "coreyms_website(url=\"https://coreyms.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261da3c7",
   "metadata": {},
   "source": [
    "# 7) Write a python program to scrape house details from mentioned URL. It should include house title, location,area, EMI and price from nobroker.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9394c949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          House_title  \\\n",
      "0   4 BHK In Independent House  For Sale  In Hebba...   \n",
      "1   4 BHK In Independent House  For Sale  In Elect...   \n",
      "2           4 BHK Flat  For Sale  In Electronic City    \n",
      "3   4 BHK Apartment  For Sale  In Nisarga Residenc...   \n",
      "4   4 BHK Flat  For Sale  In Sobha Silicon Oasis  ...   \n",
      "5   4 BHK For Sale  In Daadys Garden In Electronic...   \n",
      "6         4 BHK Flat  For Sale  In , Electronic City    \n",
      "7   4 BHK Flat  For Sale  In Hosa Road, Parappana ...   \n",
      "8   4 BHK In Independent House  For Sale  In Elect...   \n",
      "9   4 BHK In Independent House  For Sale  In Elect...   \n",
      "10  4 BHK Apartment  For Sale  In Gopalan Gardenia...   \n",
      "11       4 BHK For Sale  In Gpr Royale In Gpr Royale    \n",
      "12  4 BHK In Independent House  For Sale  In Sarja...   \n",
      "13  4 BHK Flat  For Sale  In Heena Enclave In Elec...   \n",
      "14  4 BHK For Sale  In Deccan Palms Park In Electr...   \n",
      "15  4 BHK In Independent House  For Sale  In Elect...   \n",
      "\n",
      "                                             Location        Area  \\\n",
      "0   Independent House, Bangalore - Hosur Road, Nea...  1,800 sqft   \n",
      "1                    Independent House, brand factory  2,000 sqft   \n",
      "2   Standalone Building, YOUNG LIFE PG FOR LADIES,...  1,120 sqft   \n",
      "3   Nisarga Residency  Near Thali Resturant, Anant...  2,000 sqft   \n",
      "4   Sobha Silicon Oasis Naganathapura, Rayasandra ...  1,879 sqft   \n",
      "5   Daadys Garden  Kammasandra Rd, Kammasandra, El...  2,600 sqft   \n",
      "6   Standalone Building, 16th Cross Road Neeladri ...  2,000 sqft   \n",
      "7   Standalone Building, 11th cross.anjanadri lay out  3,000 sqft   \n",
      "8               Independent House, surya nagar face 1  3,000 sqft   \n",
      "9    Independent House, Hosur Rd,Near Infosys Limited  1,200 sqft   \n",
      "10  Gopalan Gardenia  Gopalan gardenia, Veerasandr...  2,650 sqft   \n",
      "11                                          6th Cross  3,100 sqft   \n",
      "12  Independent House,  Shantipura Village , S.P L...  1,100 sqft   \n",
      "13            Neeladri Nagar,Near Pioneer Sun Blossom  2,350 sqft   \n",
      "14  Deccan Palms Park  Deccan Palms Villas, Deccan...  3,000 sqft   \n",
      "15  Independent House, Industrial Area Near Tech M...  1,500 sqft   \n",
      "\n",
      "                 EMI         Price  \n",
      "0      ₹77,374/Month  ₹1.35 Crores  \n",
      "1      ₹39,546/Month      ₹69 Lacs  \n",
      "2      ₹28,657/Month      ₹50 Lacs  \n",
      "3      ₹45,851/Month      ₹80 Lacs  \n",
      "4      ₹91,703/Month   ₹1.6 Crores  \n",
      "5      ₹85,971/Month   ₹1.5 Crores  \n",
      "6      ₹39,546/Month      ₹69 Lacs  \n",
      "7      ₹71,643/Month  ₹1.25 Crores  \n",
      "8   ₹1.43 Lacs/Month   ₹2.5 Crores  \n",
      "9      ₹42,985/Month      ₹75 Lacs  \n",
      "10     ₹68,777/Month   ₹1.2 Crores  \n",
      "11     ₹85,971/Month   ₹1.5 Crores  \n",
      "12     ₹40,120/Month      ₹70 Lacs  \n",
      "13     ₹71,643/Month  ₹1.25 Crores  \n",
      "14     ₹85,971/Month   ₹1.5 Crores  \n",
      "15     ₹57,314/Month      ₹1 Crore  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nobroker_fun(url):\n",
    "    data = requests.get(url)\n",
    "    soup = BeautifulSoup(data.content,\"html.parser\")\n",
    "    data = []\n",
    "    for i in soup.find_all('article'):\n",
    "        House_title = i.find('h2',class_=\"heading-6 font-semi-bold nb__25Cl7\").get_text()\n",
    "        Location = i.find('div',class_=\"nb__1EwQz\").get_text()\n",
    "        Area = i.find('div',class_=\"nb__FfHqA\").get_text()\n",
    "        EMI = i.find('div',id=\"roomType\").get_text()\n",
    "        scrap_price = i.find('div',id='minDeposit').get_text()\n",
    "        regex = r'₹[0-9]+,[0-9]+ per sq.ft.'\n",
    "        price = re.sub(regex,\"\",scrap_price)\n",
    "        val = {\"House_title\":House_title,\"Location\":Location,\"Area\":Area,\"EMI\":EMI,'Price':price}\n",
    "        data.append(val)\n",
    "    df = pd.DataFrame(data) \n",
    "    print(df)\n",
    "    return 'success'\n",
    "\n",
    "nobroker_fun(url='https://www.nobroker.in/property/sale/bangalore/Electronic%20City?type=BHK4&searchParam=W3sibGF0IjoxMi44N%20DUyMTQ1LCJsb24iOjc3LjY2MDE2OTUsInBsYWNlSWQiOiJDaElKdy1GUWQ0cHNyanNSSGZkYXpnXzhYRW8%20iLCJwbGFjZU5hbWUiOiJFbGVjdHJvbmljIENpdHkifV0=&propertyAge=0&radius=2.0')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c1b83",
   "metadata": {},
   "source": [
    "# 8) Write a python program to scrape mentioned details from dineout.co.in :\n",
    "    \n",
    "    i) Restaurant name\n",
    "    ii) Cuisine\n",
    "    iii) Location\n",
    "    iv) Ratings\n",
    "    v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83d683d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Restaurant_Name  \\\n",
      "0                    Castle Barbeque   \n",
      "1                    Jungle Jamboree   \n",
      "2                    Castle Barbeque   \n",
      "3                         Cafe Knosh   \n",
      "4               The Barbeque Company   \n",
      "5                        India Grill   \n",
      "6                     Delhi Barbeque   \n",
      "7   The Monarch - Bar Be Que Village   \n",
      "8                         World Cafe   \n",
      "9                  Indian Grill Room   \n",
      "10                   Mad 4 Bar B Que   \n",
      "11                       Barbeque 29   \n",
      "12                        Glasshouse   \n",
      "\n",
      "                                              Cuisine  \\\n",
      "0                               Chinese, North Indian   \n",
      "1              North Indian, Barbecue, Italian, Asian   \n",
      "2                               North Indian, Chinese   \n",
      "3    Multi-Cuisine, North Indian, Italian, Contine...   \n",
      "4            Barbecue, Chinese, Mughlai, North Indian   \n",
      "5                    North Indian, Italian, Oriental    \n",
      "6                              Barbecue, North Indian   \n",
      "7                    North Indian, Chinese, Fast Food   \n",
      "8                  North Indian, Chinese, Continental   \n",
      "9                     North Indian, Mughlai, Barbecue   \n",
      "10                              North Indian, Mughlai   \n",
      "11                    North Indian, Chinese, Barbecue   \n",
      "12   Multi-Cuisine, Asian, European, Italian, Nort...   \n",
      "\n",
      "                                             Location Ratings  \\\n",
      "0                      Connaught Place, Central Delhi     3.5   \n",
      "1              3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
      "2              Pacific Mall,Tagore Garden, West Delhi       4   \n",
      "3   The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
      "4                  Gardens Galleria,Sector 38A, Noida     4.1   \n",
      "5                Hilton Garden Inn,Saket, South Delhi     3.9   \n",
      "6      Taurus Sarovar Portico,Mahipalpur, South Delhi     3.6   \n",
      "7   Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.9   \n",
      "8    Vibe by The Lalit Traveller,Sector 35, Faridabad     4.2   \n",
      "9    Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
      "10                               Sector 29, Faridabad     3.9   \n",
      "11                                     NIT, Faridabad     4.2   \n",
      "12  DoubleTree By Hilton Gurugram Baani Square,Sec...     4.1   \n",
      "\n",
      "                                            Image_URL  \n",
      "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "12  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dineout_restaurent(url):\n",
    "    data = requests.get(url)\n",
    "    soup = BeautifulSoup(data.content,\"html.parser\")\n",
    "    data = []\n",
    "    for i in soup.find_all('div',attrs={'class':'restnt-card restaurant'}):\n",
    "        Restaurant_Name = i.find('a',href=True).get_text()\n",
    "        Cuisine = i.find('span',class_=\"double-line-ellipsis\").get_text().split('|')[1]\n",
    "        Location = i.find('div',class_=\"restnt-loc ellipsis\").get_text()\n",
    "        Ratings = i.find('div',class_=\"restnt-rating rating-4\").get_text()\n",
    "        Image_URL = i.find('img',class_='lazy-load-img no-img')['data-src']\n",
    "        val = {\"Restaurant_Name\":Restaurant_Name,\"Cuisine\":Cuisine,\"Location\":Location,\"Ratings\":Ratings,'Image_URL':Image_URL}\n",
    "        data.append(val)\n",
    "    df = pd.DataFrame(data) \n",
    "    print(df)\n",
    "    return 'success'\n",
    "dineout_restaurent(url='https://www.dineout.co.in/delhi-restaurants/buffet-special')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f202c7",
   "metadata": {},
   "source": [
    "# 9) Write a python program to scrape weather details for last 24 hours from Tutiempo.net :\n",
    "    \n",
    "    i) Hour\n",
    "    ii) Temperature\n",
    "    iii) Wind\n",
    "    iv) Weather condition\n",
    "    v) Humidity\n",
    "    vi) Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8173f1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Today                                                          \n",
      "     Hour Weather condition Temperature     Wind Humidity  Pressure\n",
      "0   20:00        Light Rain        16°C  11 km/h     100%  1017 hPa\n",
      "1   19:30        Light Rain        16°C   6 km/h     100%  1018 hPa\n",
      "2   19:00        Light Rain        16°C   7 km/h     100%  1018 hPa\n",
      "3   18:30        Light Rain        16°C   6 km/h     100%  1017 hPa\n",
      "4   18:00           Drizzle        16°C   6 km/h     100%  1017 hPa\n",
      "5   17:30        Light Rain        16°C   6 km/h     100%  1017 hPa\n",
      "6   17:00        Light Rain        17°C  11 km/h      94%  1018 hPa\n",
      "7   16:30        Light Rain        17°C  13 km/h     100%  1018 hPa\n",
      "8   16:00        Light Rain        17°C  11 km/h     100%  1017 hPa\n",
      "9   15:30           Drizzle        17°C   7 km/h     100%  1017 hPa\n",
      "10  15:00           Drizzle        18°C   7 km/h      94%     - hPa\n",
      "11  14:30           Drizzle        18°C   7 km/h      88%     - hPa\n",
      "12  14:00        Light Rain        19°C   9 km/h      83%  1017 hPa\n",
      "13  13:30        Light Rain        18°C   9 km/h      83%  1017 hPa\n",
      "14  13:00              Mist        19°C   9 km/h      78%  1017 hPa\n",
      "15  12:30              Mist        19°C   9 km/h      78%  1018 hPa\n",
      "16  12:00              Mist        18°C   9 km/h      83%  1018 hPa\n",
      "17  11:30              Mist        18°C   7 km/h      88%  1019 hPa\n",
      "18  11:00              Mist        18°C   7 km/h      88%  1019 hPa\n",
      "19  10:30              Mist        18°C   9 km/h      88%  1020 hPa\n",
      "20  10:00              Mist        17°C   7 km/h      82%  1020 hPa\n",
      "21  09:30               Fog        17°C   7 km/h      82%  1020 hPa\n",
      "22  09:00               Fog        16°C   7 km/h      88%  1019 hPa\n",
      "23  08:30               Fog        16°C   7 km/h      88%  1019 hPa\n",
      "24  08:00               Fog        15°C     Calm      88%  1018 hPa\n",
      "25  07:30               Fog        15°C   6 km/h      88%  1018 hPa\n",
      "26  07:00               Fog        15°C   9 km/h      88%  1018 hPa\n",
      "27  06:30              Mist        15°C   6 km/h      88%  1017 hPa\n",
      "28  06:00              Mist        15°C   7 km/h      88%  1017 hPa\n",
      "29  05:30              Mist        15°C   6 km/h      88%  1016 hPa\n",
      "30  05:00              Mist        15°C   6 km/h      88%  1016 hPa\n",
      "31  04:30              Mist        16°C   7 km/h      82%  1016 hPa\n",
      "32  04:00              Mist        16°C   6 km/h      82%  1016 hPa\n",
      "33  03:30              Mist        16°C   6 km/h      82%  1016 hPa\n",
      "34  03:00              Mist        16°C   6 km/h      82%  1016 hPa\n",
      "35  02:30              Mist        16°C   6 km/h      82%  1017 hPa\n",
      "36  02:00              Mist        16°C   6 km/h      82%  1017 hPa\n",
      "37  01:30              Mist        16°C   6 km/h      82%  1017 hPa\n",
      "38  01:00              Mist        17°C     Calm      82%  1017 hPa\n",
      "39  00:30              Mist        17°C     Calm      82%  1017 hPa\n",
      "40  00:00              Mist        17°C     Calm      82%  1017 hPa\n",
      "41  23:30              Mist        17°C     Calm      82%  1017 hPa\n",
      "42  23:00              Mist        17°C     Calm      82%  1017 hPa\n",
      "43  22:30              Mist        18°C     Calm      83%  1017 hPa\n",
      "44  22:00              Mist        18°C     Calm      83%  1017 hPa\n",
      "45  21:30              Mist        18°C     Calm      83%  1017 hPa\n",
      "46  21:00              Mist        18°C     Calm      83%  1017 hPa\n",
      "47  20:30              Mist        18°C   6 km/h      83%  1017 hPa\n",
      "48  20:00              Mist        19°C     Calm      78%  1017 hPa\n",
      "49  19:30              Mist        19°C     Calm      78%  1016 hPa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weather_details(url):\n",
    "    tables = pd.read_html(url)\n",
    "    df = pd.DataFrame(tables[3])\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop([df.index[41],df.index[42]])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.rename(columns={\"Weather conditionWC\":'Weather condition','Tem.':'Temperature','Hum.':\"Humidity\"},inplace=True)\n",
    "    print(df)\n",
    "    return 'success'\n",
    "weather_details(url=\"https://en.tutiempo.net/delhi.html?data=last-24-hours\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c354bdf",
   "metadata": {},
   "source": [
    "# 10) Write a python program to scrape monument name, monument description, image URL about top 10 monuments from puredestinations.co.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afa0161b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Monument_Name  \\\n",
      "0                             Taj Mahal, Agra   \n",
      "1  Golden Temple (Harmandir Sahib), Amritsar    \n",
      "2                   Meenakshi Temple, Madurai   \n",
      "3                       Mysore Palace, Mysore   \n",
      "4                    Gateway of India, Mumbai   \n",
      "5                         Red Fort, New Delhi   \n",
      "6                          Hawa Mahal, Jaipur   \n",
      "7                      Qutub Minar, New Delhi   \n",
      "8                        Sanchi Stupa, Sanchi   \n",
      "9                        Charminar, Hyderabad   \n",
      "\n",
      "                                Monument_Description  \\\n",
      "0  Enlisted in the Seven Wonders of the World, Th...   \n",
      "1  The holiest shrine and pilgrimage place locate...   \n",
      "2  Meenakshi Temple is situated on the Southern b...   \n",
      "3  The Mysore Palace is a famous historical monum...   \n",
      "4  Even though Mumbai is famous for its Bollywood...   \n",
      "5  Declared as the UNESCO’s World Heritage Site, ...   \n",
      "6  Explore a blend of beauty and Rajasthan cultur...   \n",
      "7  Discover one of the tallest towers in the worl...   \n",
      "8  The beautiful and massive dome, Sanchi Stupa a...   \n",
      "9  No visit to Hyderabad should be complete witho...   \n",
      "\n",
      "                                          Images_Url  \n",
      "0  http://www.puredestinations.co.uk/wp-content/u...  \n",
      "1  http://www.puredestinations.co.uk/wp-content/u...  \n",
      "2  http://www.puredestinations.co.uk/wp-content/u...  \n",
      "3  http://www.puredestinations.co.uk/wp-content/u...  \n",
      "4  http://www.puredestinations.co.uk/wp-content/u...  \n",
      "5  http://www.puredestinations.co.uk/wp-content/u...  \n",
      "6  http://www.puredestinations.co.uk/wp-content/u...  \n",
      "7  http://www.puredestinations.co.uk/wp-content/u...  \n",
      "8  http://www.puredestinations.co.uk/wp-content/u...  \n",
      "9  http://www.puredestinations.co.uk/wp-content/u...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def monuments(url):\n",
    "    data = requests.get(url)\n",
    "    soup = BeautifulSoup(data.content,\"html.parser\")\n",
    "    all_p = [i.get_text() for i in soup.find_all('p')]\n",
    "    final_p = all_p[4:33]\n",
    "    f_li = list(filter(lambda x:x!='',final_p))\n",
    "    Monument_Name = f_li[::2]\n",
    "    Monument_Description = f_li[1::2]\n",
    "    Images_Url = []\n",
    "    for n,i in enumerate(soup.find_all('img',class_='alignnone')):\n",
    "        try:\n",
    "            Images_Url.append(i['data-src'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    df = pd.DataFrame(list(zip(Monument_Name,Monument_Description,Images_Url)),columns=['Monument_Name','Monument_Description','Images_Url'])\n",
    "    print(df)\n",
    "    return 'success'\n",
    "monuments(url='https://www.puredestinations.co.uk/top-10-famous-monuments-to-visit-in-india/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277a1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
